<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model description for glca â€¢ glca</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Model description for glca">
<meta property="og:description" content="glca">
<meta property="og:image" content="http://kim0sun.github.io/glca/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">glca</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">1.4.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/glca.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/model-description.html">Model description for glca</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/kim0sun/glca/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Model description for glca</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/kim0sun/glca/blob/HEAD/vignettes/model-description.Rmd" class="external-link"><code>vignettes/model-description.Rmd</code></a></small>
      <div class="hidden name"><code>model-description.Rmd</code></div>

    </div>

    
    
<p>Suppose that there are <span class="math inline">\(G\)</span> groups, and the <span class="math inline">\(g\)</span>th group consists of <span class="math inline">\(n_g\)</span> observations for <span class="math inline">\(g=1,\ldots,G\)</span>, and there are <span class="math inline">\(M\)</span> categorical manifest items, where the <span class="math inline">\(m\)</span>th item has <span class="math inline">\(r_m\)</span> categories for <span class="math inline">\(m=1,\ldots,M\)</span>. Let <span class="math inline">\(\mathbf{Y}_{ig}=(Y_{ig1}, \dots, Y_{igM})^\top\)</span> and <span class="math inline">\({\bf y}_{ig} = (y_{ig1}, \ldots, y_{igM})^\top\)</span> denote a set of item variables and their responses given by the <span class="math inline">\(i\)</span>th individual within the <span class="math inline">\(g\)</span>th group, respectively. The number of possible response patterns of <span class="math inline">\(\mathbf{Y}_{ig}\)</span> is <span class="math inline">\(\prod_{m = 1}^{M}r_m\)</span>, and it is likely that most of these response patterns are sparse. The multiple-group LCA assumes that associations among manifest items can be explained by the latent classifier <span class="math inline">\(L_{ig}\)</span>, where <span class="math inline">\(L_{ig}\)</span> is the latent class variable having <span class="math inline">\(C\)</span> categories for the <span class="math inline">\(i\)</span>th individual within the <span class="math inline">\(g\)</span>th group. To reflect multiple-group data structures, we discuss two different LCA approaches, namely fixed-effect and random-effect LCA.</p>
<div class="section level2">
<h2 id="models">Models<a class="anchor" aria-label="anchor" href="#models"></a>
</h2>
<div class="section level3">
<h3 id="fixed-effect-latent-class-analysis">Fixed-effect latent class analysis<a class="anchor" aria-label="anchor" href="#fixed-effect-latent-class-analysis"></a>
</h3>
<p>The fixed-effect LCA can reflect group differences in latent structure by specifying an LCR model for a given subgroup. We extend the simultaneous LCA (Clogg &amp; Goodman, 1985) by incorporating logistic regression in the class prevalence and refer it to <em>multiple-group latent class regression</em> (mgLCR).
Let <span class="math inline">\({\bf x}_{ig}=(x_{ig1}, \ldots, x_{igp})^\top\)</span> be a subject-specific <span class="math inline">\(p\times 1\)</span> vector of covariates for the <span class="math inline">\(i\)</span>th individual within the <span class="math inline">\(g\)</span>th group, either discrete or continuous. Then, the observed-data likelihood of mgLCR for the <span class="math inline">\(i\)</span>th individual within the <span class="math inline">\(g\)</span>th group can be specified as
<!--------------->
<span class="math display" id="eq:likeli">\[\begin{eqnarray}
\mathcal{L}_{ig} &amp;=&amp; \sum_{c=1}^C P(\mathbf{Y}_{ig} = \mathbf{y}_{ig}, L_{ig} = c \mid {\bf x}_{ig}) \nonumber \\
&amp;=&amp; \sum_{c=1}^C \left[ P(L_{ig} = c \mid {\bf x}_{ig}) \prod_{m = 1}^{M} P(y_{igm} = k \mid L_{ig} = c) \right] \nonumber  \\
&amp;=&amp; \sum_{c=1}^C \left[ \gamma_{c \mid g}({\bf x}_{ig}) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m} \rho_{mk \mid cg}^{I(y_{igm} = k)} \right],
\tag{1}
\end{eqnarray}\]</span>
<!--------------->
where <span class="math inline">\(I(y_{igm}=k)\)</span> is an indicator function that is equal to 1 when the response to the <span class="math inline">\(m\)</span>th item from the <span class="math inline">\(i\)</span>th individual within the <span class="math inline">\(g\)</span>th group is <span class="math inline">\(k\)</span> and is otherwise equal to 0. The likelihood given in <a href="#eq:likeli">(1)</a> contains two types of parameters:</p>
<!-- %\item $\rho_{mk \mid cg} = P(Y_{igm} = k \mid L_{ig} = c)$  -->
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\rho_{mk \mid cg}\)</span> represents the probability of an individual within the <span class="math inline">\(g\)</span>th group responding <span class="math inline">\(k\)</span> to the <span class="math inline">\(m\)</span>th item given his or her latent class as <span class="math inline">\(c\)</span>.
<!-- %\item $\gamma_{c \mid g}({\bf x}_{ig}) =  P(L_{ig} = c \mid {\bf x}_{ig})$  -->
</li>
<li>
<span class="math inline">\(\gamma_{c \mid g}({\bf x}_{ig})\)</span> is the probability of the <span class="math inline">\(i\)</span>th individual belonging to the latent class <span class="math inline">\(c\)</span> within the <span class="math inline">\(g\)</span>th group, which could be influenced by the subject-specific covariates <span class="math inline">\({\bf x}_{ig}\)</span>.</li>
</ol>
<p>The <span class="math inline">\(\rho\)</span>-parameter is the measurement parameter in mgLCR (i.e., item-response probability), describing a tendency of individuals in a latent class <span class="math inline">\(c\)</span> to respond to the <span class="math inline">\(m\)</span>th item for <span class="math inline">\(m = 1,\ldots,M\)</span>. Comparison of estimated item-response probabilities across groups is a valuable strategy for quantifying measurement invariance because they solely determine the meaning of the latent class. By comparing the model fit with the parameter held constant across groups (i.e., <span class="math inline">\(\rho_{mk \mid c} = \rho_{mk \mid c1} = \dots = \rho_{mk \mid cG}\)</span> for <span class="math inline">\(k=1,\ldots,r_m\)</span>, <span class="math inline">\(m=1,\ldots,M\)</span>, and <span class="math inline">\(c=1,\ldots,C\)</span>) against an alternative model with freely varying parameters, we obtain evidence on whether measurement invariance across groups can be assumed. As given in <a href="#eq:likeli">(1)</a>, the subject-specific covariates <span class="math inline">\({\bf x}_{ig}\)</span> may influence the probability of the individual belonging to a specific class in the form of logistic regression as
<!--------------->
<span class="math display" id="eq:mgLCA-reg">\[\begin{eqnarray}
\gamma_{c \mid g}(\mathbf{x}_{ig}) = P(L_{ig} = c \mid {\bf x}_{ig}) = \frac{\exp(\alpha_{c \mid g}+{\bf x}_{ig}^\top \boldsymbol{\beta}_{c \mid g})}{\sum_{c'=1}^C\exp(\alpha_{c' \mid g}+{\bf x}_{ig}^\top \boldsymbol{\beta}_{c' \mid g})},
\tag{2}
\end{eqnarray}\]</span>
<!--------------->
where the coefficient vector <span class="math inline">\(\boldsymbol{\beta}_{c \mid g} = (\beta_{1c \mid g}, \ldots, \beta_{pc \mid g})^\top\)</span> can be interpreted as the expected change in the log odds of belonging to a class <span class="math inline">\(c\)</span> versus belonging to the referent class <span class="math inline">\(C\)</span> (i.e., <span class="math inline">\(\alpha_{C \mid g}=0\)</span> and <span class="math inline">\(\boldsymbol{\beta}_{C \mid g} = {\bf 0}\)</span> for <span class="math inline">\(g=1, \ldots, G\)</span>).
Then, the observed log-likelihood function for the mgLCR model can be specified as
<!--------------->
<span class="math display" id="eq:loglik-mgLCA">\[\begin{equation}
\ell_{mgLCR} = \sum_{g = 1}^{G}\sum_{i = 1}^{n_{g}} \log \mathcal{L}_{ig}.
\tag{3}
\end{equation}\]</span>
<!--------------->
It should be noted that, similar to item-response probabilities, coefficients of logistic regression can be constrained to be equal across subgroups (i.e., <span class="math inline">\(\boldsymbol{\beta}_{c} = \boldsymbol{\beta}_{c \mid 1}= \cdots = \boldsymbol{\beta}_{c \mid G}\)</span> for <span class="math inline">\(c=1, \ldots,C\)</span>) to test whether the effects of covariates are identical across groups.</p>
</div>
<div class="section level3">
<h3 id="random-effect-latent-class-analysis">Random-effect latent class analysis<a class="anchor" aria-label="anchor" href="#random-effect-latent-class-analysis"></a>
</h3>
<!-- %As in Section~\ref{intro}, there are parametric and nonparametric approaches for random-effect LCA.  -->
<p>The random-effect LCA considers the group variation in the latent class prevalence for each group using random coefficients, for example,
<!-- %For example, the prevalence of latent class for each group can be modeled using random coefficients $\boldsymbol{\lambda}=(\lambda_1, \ldots, \lambda_G)^\top$ as  -->
<!--------------->
<span class="math display">\[
P(L_{ig} = c) =
\frac{\exp(\alpha_{c} + \sigma_{c} \lambda_{g})}{\sum_{c' = 1}^{C}\exp(\alpha_{c'} + \sigma_{c'} \lambda_{g})},
\]</span>
<!--------------->
where <span class="math inline">\(\boldsymbol{\lambda}=(\lambda_1, \ldots, \lambda_G)^\top\)</span> represents group variation in the class prevalence.
<!-- %For random-effect LCA, there are parametric and nonparametric approaches to random coefficients \cite{vermunt2003}.  -->
In the parametric random-effect LCA, the random coefficients are assumed to be derived from parametric distributions such as standard normal distribution. However, the nonparametric approach assumes no specific distribution; rather, it only assumes that random coefficients follow a specific probability mass function with some mass points. In other words, the nonparametric approach employs categorical level-2 latent variable (i.e., latent cluster) <span class="math inline">\(U_{g}\)</span> whose probability mass function is <span class="math inline">\(P(U_g=w) = \delta_w\)</span> for <span class="math inline">\(w=1,\ldots,W\)</span>. Using the classification mechanics of LCA, the latent cluster membership of level-2 units can be identified by the small number of representative patterns of class prevalences in multiple groups. Therefore, the meaning of the <span class="math inline">\(w\)</span>th level of latent cluster variable is determined by the prevalence of latent classes <span class="math inline">\(P(L_{ig} = c \mid U_{g} = w)\)</span> for <span class="math inline">\(c=1,\ldots,C\)</span>. Considering latent cluster variable as a group variable, the nonparametric approach provides more meaningful interpretations in group comparison than parametric approach; we can examine whether the latent class structure differs across latent cluster memberships. Therefore, we focus on the nonparametric random-effect LCA, hereafter referred to as <em>nonparametric latent class regression</em> (npLCR).</p>
<!-- %In npLCR, the latent cluster variable is used to specify the prevalence of latent class as  -->
<!--------------->
<!-- %\gamma_{ig(c\mid w)} =  -->
<!-- %\[ -->
<!-- %P(L_{ig} = c \mid U_{g} = w) =  -->
<!-- %\frac{\exp(\alpha_{c \mid w})}{\sum_{c' = 1}^{C}\exp(\alpha_{c' \mid w})}, -->
<!-- %\] -->
<!-- %\end{eqnarray}  -->
<!--------------->
<!-- %where the probability mass function for the random coefficient $\alpha_{c \mid w}$ is $P(U_g=w) = \delta_w$ for $w=1,\ldots,W$, which can be interpreted as latent cluster prevalence.  -->
<p>The observed-data likelihood of npLCR for the <span class="math inline">\(g\)</span>th group can be expressed by
<!--------------->
<span class="math display" id="eq:grouploglik-mLCA">\[\begin{eqnarray}
\mathcal{L}_{g} &amp;=&amp;
\sum_{w = 1}^{W} P(U_g = w) \prod_{i=1}^{n_g} \left\{\sum_{c = 1}^{C} P(Y_{ig}=y_{ig}, L_{ig} = c \mid U_{g} = w, \mathbf{x}_{ig}, \mathbf{z}_{g})\right\} \nonumber \\
&amp;=&amp; \sum_{w = 1}^{W} P(U_g = w) \prod_{i=1}^{n_g} \left\{\sum_{c = 1}^{C} P(L_{ig} = c \mid U_{g} = w, \mathbf{x}_{ig}, \mathbf{z}_{g}) \prod_{m=1}^M P(Y_{igm}=k \mid L_{ig} = c )  \right\} \nonumber \\
&amp;=&amp; \sum_{w = 1}^{W} \delta_{w} \prod_{i=1}^{n_g}  \left\{ \sum_{c = 1}^{C} \gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_{g}) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m}\rho_{mk \mid c}^{I(y_{igm} = k)}\right\},
\tag{4}
\end{eqnarray}\]</span>
<!--------------->
where <span class="math inline">\({\bf x}_{ig} = (x_{ig1}, \ldots, x_{igp})^\top\)</span> and <span class="math inline">\({\bf z}_g=(z_{g1}, \ldots, z_{gq})^\top\)</span> denote vectors of subject-specific (i.e., level-1) and group-specific (i.e., level-2) covariates for <span class="math inline">\(i=1, \ldots, n_g\)</span> and <span class="math inline">\(g=1,\ldots,G\)</span>, respectively.
<!-- %where $I(y_{igm}=k)$ is an indicator function that is equal to 1 when the response to the $m$th item from the $i$th individual within the $g$th group is $k$ and is otherwise equal to 0.  -->
The likelihood given in <a href="#eq:grouploglik-mLCA">(4)</a> contains three types of parameters:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\rho_{mk \mid c}\)</span> represents the probability of an individual responding <span class="math inline">\(k\)</span> to the <span class="math inline">\(m\)</span>th item given his or her latent class as <span class="math inline">\(c\)</span>.</li>
<li>
<span class="math inline">\(\gamma_{c \mid w}({\bf x}_{ig}, {\bf z}_g)\)</span> is the probability of the <span class="math inline">\(i\)</span>th individual within the <span class="math inline">\(g\)</span>th group belonging to the latent class <span class="math inline">\(c\)</span> given the latent cluster <span class="math inline">\(w\)</span>, which could be influenced by the subject-specific covariates <span class="math inline">\({\bf x}_{ig}\)</span> and the group-specific covariates <span class="math inline">\({\bf z}_g\)</span>.</li>
<li>
<span class="math inline">\(\delta_w\)</span> represents the latent cluster prevalence for <span class="math inline">\(w=1,\ldots,W\)</span>.</li>
</ol>
<!-- %In npLCR we can incorporate level-1 and/or level-2 covariates to predict latent class membership.  --><p>The class prevalence can be modeled using the logistic regression as
<!--------------->
<span class="math display" id="eq:MLCA-reg">\[\begin{eqnarray}
\tag{5}
\gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_{g}) = P(L_{ig} = c \mid U_{g} = w, \mathbf{x}_{ig}, \mathbf{z}_{g}) =
\frac{\exp(\alpha_{c \mid w} + \mathbf{x}^{\top}_{ig}\boldsymbol{\beta}_{1c \mid w} + \mathbf{z}^{\top}_{g}\boldsymbol{\beta}_{2c})}
{\sum_{c' = 1}^{C} \exp(\alpha_{c' \mid w} + \mathbf{x}^{\top}_{ig}\boldsymbol{\beta}_{1c' \mid w} + \mathbf{z}^{\top}_{g}\boldsymbol{\beta}_{2c'})},
\end{eqnarray}\]</span>
<!--------------->
where vectors <span class="math inline">\(\boldsymbol{\beta}_{1c \mid w} = (\beta_{11c \mid w}, \ldots, \beta_{1pc \mid w})^\top\)</span> and <span class="math inline">\(\boldsymbol{\beta}_{2c} = (\beta_{21c}, \ldots, \beta_{2qc})^\top\)</span> are logistic regression coefficients for level-1 and level-2 covariates, respectively.
Then, the observed log-likelihood of npLCR is specified as
<!--------------->
<span class="math display" id="eq:loglik-mLCA">\[\begin{eqnarray}
\tag{6}
\ell_{npLCR} = \sum_{g = 1}^{G} \log \mathcal{L}_{g}.
\end{eqnarray}\]</span>
<!---------------></p>
<p>Note that coefficients for level-1 covariates depend on both latent classes and clusters, while coefficients for level-2 covariates depend only on latent class membership. We may refer the model <a href="#eq:MLCA-reg">(5)</a> to the random slope model as coefficients for level-1 covariates are different across latent clusters. The coefficients for level-1 covariates can be constrained to be equal across clusters (i.e., <span class="math inline">\(\boldsymbol{\beta}_{1c} = \boldsymbol{\beta}_{1c \mid 1} = \cdots = \boldsymbol{\beta}_{1c \mid W}\)</span> for <span class="math inline">\(c=1, \ldots,C\)</span>) to test whether the effects of level-1 covariates are identical across all latent cluster memberships.
It should also be noted that the measurement invariance is assumed across latent cluster memberships in npLCR (i.e., <span class="math inline">\(\rho_{mk \mid c} = \rho_{mk \mid c1} = \dots = \rho_{mk \mid cW}\)</span> for <span class="math inline">\(k=1,\ldots,r_m\)</span>, <span class="math inline">\(m=1,\ldots,M\)</span>, and <span class="math inline">\(c=1,\ldots,C\)</span>). If not, the item response probability may vary across latent cluster memberships, suggesting that the latent class structure itself is different between latent clusters. Thus, it no longer makes sense to use latent class prevalences as identifiers for the latent cluster membership.
<!-- %It should also be noted that we should assume measurement invariance across latent cluster memberships (i.e., $\rho_{mk \mid c} = \rho_{mk \mid c1} = \dots = \rho_{mk \mid cW}$ for $k=1,\ldots,r_m$, $m=1,\ldots,M$, and $c=1,\ldots,C$) in npLCR because latent cluster membership is identified by latent class prevalences of each group. If item response probabilities vary across latent clusters, the meaning of latent classes varies from cluster to cluster. Then, it no longer makes sense to use latent class prevalences as identifiers for latent clusters. --></p>
</div>
</div>
<div class="section level2">
<h2 id="estimation">Estimation<a class="anchor" aria-label="anchor" href="#estimation"></a>
</h2>
<div class="section level3">
<h3 id="fixed-effect-latent-class-analysis-1">Fixed-effect latent class analysis<a class="anchor" aria-label="anchor" href="#fixed-effect-latent-class-analysis-1"></a>
</h3>
<p>The package <code>glca</code> finds the maximum-likelihood (ML) estimates for mgLCR and npLCR using expectation-maximization (EM) algorithm (Dempster et al., 1977). The EM algorithm iterates two steps: expectation step (E-step) and maximization step (M-step) in order to find the solution maximizing the log-likelihood functions given in <a href="#eq:loglik-mgLCA">(3)</a> and <a href="#eq:loglik-mLCA">(6)</a>.</p>
<p>For mgLCR, E-step computes the posterior probabilities
<!--------------->
<span class="math display" id="eq:post">\[\begin{eqnarray*}
\tag{7}
  \theta_{ig(c)} = P(L_{ig} = c \mid \mathbf{Y}_{ig} = \mathbf{y}_{ig}, \mathbf{x}_{ig})
  = \frac{\gamma_{c \mid g}(\mathbf{x}_{ig}) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m} \rho_{mk \mid cg}^{I(y_{igm} = k)}}{\sum_{c' = 1}^{C} \gamma_{c' \mid g}(\mathbf{x}_{ig}) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m} \rho_{mk \mid c'g}^{I(y_{igm} = k)}}\\
\end{eqnarray*}\]</span>
<!--------------->
with current estimates for <span class="math inline">\(i=1, \ldots, n_g\)</span>, <span class="math inline">\(g=1,\ldots,G\)</span>, and <span class="math inline">\(c=1,\ldots,C\)</span>. M-step maximizes the complete-data likelihood (i.e., the likelihood for the cross-classification by <span class="math inline">\(L_{ig}\)</span> and <span class="math inline">\(\mathbf{y}_{ig}\)</span>) with respect to <span class="math inline">\(\beta\)</span>- and <span class="math inline">\(\rho\)</span>-parameters. In particular, when all values of <span class="math inline">\(\theta_{ig(c)}\)</span> are known, updated estimates for <span class="math inline">\(\beta\)</span>-parameters can be calculated by the Newton-Raphson algorithm for multinomial logistic regression given in <a href="#eq:mgLCA-reg">(2)</a>, provided that the computational routine allows fractional responses rather than integer counts (Bandeen-Roche et al., 1997). Therefore, the package <code>glca</code> conducts one-cycle of Newton-Raphson algorithm to update <span class="math inline">\(\beta\)</span>-parameters at every iteration in M-step. If there is no covariate in the model, the class prevalence can be updated directly without estimating <span class="math inline">\(\beta\)</span>-parameters as <span class="math inline">\(\hat{\gamma}_{c \mid g} = P(L_{ig}=c) = \sum_{i=1}^{n_g} \theta_{ig(c)}/n_g\)</span> for <span class="math inline">\(c=1,\ldots,C\)</span> and <span class="math inline">\(g=1,\ldots,G\)</span>. The item-response probabilities, <span class="math inline">\(\rho_{mk \mid cg}\)</span> can be interpreted as parameters in a multinomial distribution when <span class="math inline">\(\theta_{ig(c)}\)</span> is known, so we have
<!--------------->
<span class="math display">\[
\hat{\rho}_{mk \mid cg} = \frac{\sum_{i=1}^{n_g} \theta_{ig(c)}I(y_{igm} = k)}{\sum_{i=1}^{n_g}  \theta_{ig(c)}}
\]</span>
<!--------------->
for <span class="math inline">\(k=1, \ldots, r_m\)</span>, <span class="math inline">\(m=1,\ldots,M\)</span>, <span class="math inline">\(c=1,\ldots,C\)</span>, and <span class="math inline">\(g=1,\ldots,G\)</span>. Under the measurement invariance assumption (i.e., <span class="math inline">\(\rho_{mk \mid c} = \rho_{mk \mid c1} = \dots = \rho_{mk \mid cG}\)</span>), the <span class="math inline">\(\rho\)</span>-parameter will be updated as
<!--------------->
<span class="math display">\[
\hat{\rho}_{mk \mid c} = \frac{\sum_{g = 1}^{G} \sum_{i = 1}^{n_g} \theta_{ig(c)}I(y_{igm} = k)}{\sum_{g = 1}^{G} \sum_{i = 1}^{n_g} \theta_{ig(c)}}
\]</span>
<!--------------->
for <span class="math inline">\(k=1, \ldots, r_m\)</span>, <span class="math inline">\(m=1,\ldots,M\)</span>, and <span class="math inline">\(c=1,\ldots,C\)</span>.</p>
</div>
<div class="section level3">
<h3 id="random-effect-latent-class-analysis-1">Random-effect latent class analysis<a class="anchor" aria-label="anchor" href="#random-effect-latent-class-analysis-1"></a>
</h3>
<p>For npLCR, E-step involves the joint posterior probability
<!--------------->
<span class="math display" id="eq:full-post">\[\begin{eqnarray}
\tag{8}
\theta_{g(w, c_1, \ldots, c_{n_g})} =  P(U_g = w, L_{1g}=c_{1},  \ldots, L_{n_gg}=c_{n_g} \mid \mathbf{Y}_g = \mathbf{y}_g, \mathbf{x}_g, \mathbf{z}_g),
\end{eqnarray}\]</span>
<!--------------->
where <span class="math inline">\(\mathbf{y}_g = (\mathbf{y}_{1g}^\top, \ldots, \mathbf{y}_{n_gg}^\top)^\top\)</span> and <span class="math inline">\(\mathbf{x}_g = (\mathbf{x}_{1g}^\top, \ldots, \mathbf{x}_{n_gg}^\top)^\top\)</span> are all observations for <span class="math inline">\(M\)</span> manifest items and <span class="math inline">\(p\)</span> subject-specific covariates from the <span class="math inline">\(g\)</span>th group, respectively.
As shown in <a href="#eq:full-post">(8)</a>, computational complexity increases exponentially as the number of individuals per group <span class="math inline">\(n_g\)</span> increases in E-step for npLCR;
<!-- %\textcolor{orange}{The E-step in npLCR poses a problem of computational complexity as the number of individuals per group $n_g$.}  -->
when the model has <span class="math inline">\(W\)</span> latent clusters and <span class="math inline">\(C\)</span> latent classes, the implementation of the E-step would yield dimensional complexity proportional to <span class="math inline">\(W \times C^{n_g}\)</span> for each group. It is not computationally feasible even with a moderate number of individuals per group. Besides, M-step for npLCR requires only some marginal versions of posterior probabilities rather than the joint posterior probability given in <a href="#eq:full-post">(8)</a>. Therefore, the E-step can be modified to alleviate the computational complexity by accommodating the hierarchical structure of npLCR. Vermunt, 2003 proposed the upward-downward (UD) algorithm for calculating the marginal posterior probability directly in the E-step. The UD algorithm is similar to the forward-backward (FB) algorithm for handling hidden Markov models (Baum et al., 1970; Juang &amp; Rabiner, 1991). In the UD algorithm, marginal posterior probabilities <span class="math inline">\(\theta_{ig(w,c)}\)</span> can be calculated as
<!--------------->
<span class="math display" id="eq:UD">\[\begin{eqnarray}
\theta_{ig(w,c)} &amp;=&amp; P(U_g = w, L_{ig} = c \mid \mathbf{Y}_g=\mathbf{y}_g, \mathbf{x}_g, \mathbf{z}_g) \nonumber \\
&amp;=&amp; P(U_g = w \mid \mathbf{Y}_g=\mathbf{y}_g, \mathbf{x}_g, \mathbf{z}_g) P(L_{ig} = c \mid U_g = w, \mathbf{Y}_{ig}=\mathbf{y}_{ig}, \mathbf{x}_{ig}, \mathbf{z}_g) \nonumber \\
&amp;=&amp; \theta_{g(w)} \theta_{ig(c \mid w)}
\tag{9}
\end{eqnarray}\]</span>
<!--------------->
for <span class="math inline">\(i=1, \ldots, n_g\)</span>, <span class="math inline">\(g=1,\ldots,G\)</span>, <span class="math inline">\(c=1,\ldots,C\)</span>, and <span class="math inline">\(w=1,\ldots,W\)</span>. The marginal posterior probability, <span class="math inline">\(\theta_{ig(w,c)}\)</span> is the product of <em>upward probability</em>, <span class="math inline">\(\theta_{g(w)}\)</span> and <em>downward probability</em>, <span class="math inline">\(\theta_{ig(c \mid w)}\)</span>. In <a href="#eq:UD">(9)</a>, it should be noted that an individualâ€™s class membership is assumed to depend only on his/her observations, which can be presented as <span class="math inline">\(P(L_{ig} = c \mid U_g = w, \mathbf{Y}_{g}=\mathbf{y}_{g}, \mathbf{x}_g, \mathbf{z}_g) = P(L_{ig} = c \mid U_g = w, \mathbf{Y}_{ig}=\mathbf{y}_{ig}, \mathbf{x}_{ig}, \mathbf{z}_g)\)</span> for <span class="math inline">\(i=1,\ldots,n_g\)</span>. Then, the upward and downward probabilities are easily calculated as
<!--------------->
<span class="math display">\[\begin{eqnarray*}
\theta_{g(w)}
&amp;=&amp; \frac{\delta_{w} \prod_{i = 1}^{n_g} \left\{ \sum_{c = 1}^{C} \gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_g) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m}\rho_{mk \mid c}^{I(y_{igm} = k)}\right\}  }
{\sum_{w = 1}^{W} \delta_{w} \prod_{i = 1}^{n_g} \left\{ \sum_{c = 1}^{C} \gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_g) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m}\rho_{mk \mid c}^{I(y_{igm} = k)}\right\}}\;\; \mbox{and} \\
\theta_{ig(c \mid w)}
  &amp;=&amp; \frac{\gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_g) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m}\rho_{mk \mid c}^{I(y_{igm} = k)}}
  {\sum_{c' = 1}^C\gamma_{c' \mid w}(\mathbf{x}_{ig}, \mathbf{z}_g) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m}\rho_{mk \mid c'}^{I(y_{igm} = k)}}
\end{eqnarray*}\]</span>
<!--------------->
with current estimates, respectively.
<!-- %The downward probability can be obtained as -->
<!--------------->
<!-- %\begin{eqnarray*} -->
<!-- %\theta_{iwg(c)} &=& -->
<!-- %  \frac{P(L_{ig} = c, \mathbf{Y}_{ig} = \mathbf{y}_{ig} \mid U_g = w, \mathbf{x}_{ig}, \mathbf{z}_g)} -->
<!-- %  {\sum_{c' = 1}^{C}P(L_{ig} = c', \mathbf{Y}_{ig} = \mathbf{y}_{ig} \mid U_g = w, \mathbf{x}_{ig}, \mathbf{z}_g)}\nonumber \\ -->
<!-- %  &=& \frac{\gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_g) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m}\rho_{mk \mid c}^{I(y_{igm} = k)}} -->
<!-- %  {\sum_{c' = 1}^{K}\gamma_{c' \mid w}(\mathbf{x}_{ig}, \mathbf{z}_g) \prod_{m = 1}^{M}\prod_{k = 1}^{r_m}\rho_{mk \mid c'}^{I(y_{igm} = k)}} -->
<!-- %\end{eqnarray*}  -->
<!--------------->
<!-- %with current estimates.  -->
M-step maximizes the complete-data likelihood (i.e., the likelihood for the cross-classification by <span class="math inline">\(U_g\)</span>, <span class="math inline">\(L_{ig}\)</span> and <span class="math inline">\(\mathbf{y}_{ig}\)</span>) with respect to <span class="math inline">\(\boldsymbol{\beta}_{1c \mid w}\)</span>, <span class="math inline">\(\boldsymbol{\beta}_{2c}\)</span>, and <span class="math inline">\(\rho_{mk \mid c}\)</span>. In particular, when <span class="math inline">\(\theta_{ig(w,c)}\)</span> is known, updated estimates for <span class="math inline">\(\beta\)</span>-parameters can be calculated by Newton-Raphson algorithm for multinomial logistic regression given in <a href="#eq:MLCA-reg">(5)</a>, provided that the computational routine allows fractional responses rather than integer counts (Bandeen-Roche et al., 1997). Therefore, the package <code>glca</code> conducts one-cycle of Newton-Raphson algorithm to update <span class="math inline">\(\beta\)</span>-parameters at every iteration in M-step. If there is no covariate in the model, the class prevalence can be updated directly without estimating <span class="math inline">\(\beta\)</span>-parameters as <span class="math inline">\(\hat{\gamma}_{c \mid w} = P(L_{ig}=c \mid U_g =w) = \sum_{g=1}^G \sum_{i=1}^{n_g} \theta_{ig(w,c)}/\sum_{g=1}^G \theta_{g(w)}\)</span> for <span class="math inline">\(c=1,\ldots,C\)</span> and <span class="math inline">\(w=1,\ldots, W\)</span>.
The cluster prevalence <span class="math inline">\(\delta_w\)</span> and the item-response probabilities <span class="math inline">\(\rho_{mk \mid c}\)</span> can be interpreted as parameters in multinomial distributions, so we have
<!--------------->
<span class="math display" id="eq:m-nplcr">\[\begin{eqnarray}
\hat{\delta}_{w} = \frac{\sum_{g = 1}^{G} \theta_{g(w)}}{G}
\;\; \text{and}
\;\; \hat{\rho}_{mk \mid c} = \frac{\sum_{g = 1}^{G} \sum_{i = 1}^{n_g} \theta_{ig(c)}I(y_{igm} = k)}{\sum_{g = 1}^{G} \sum_{i = 1}^{n_g} \theta_{ig(c)}}
\tag{10}
\end{eqnarray}\]</span>
<!--------------->
for <span class="math inline">\(w=1,\ldots,W\)</span>, <span class="math inline">\(c=1,\ldots,C\)</span>, <span class="math inline">\(k=1,\ldots,r_m\)</span>, and <span class="math inline">\(m=1,\ldots,M\)</span>. The marginal posterior probability used in <a href="#eq:m-nplcr">(10)</a>$ are easily obtained by <span class="math inline">\(\theta_{ig(c)} = \sum_{w = 1}^{W} \theta_{ig(w,c)}\)</span>.</p>
</div>
<div class="section level3">
<h3 id="handling-missing-data">Handling missing data<a class="anchor" aria-label="anchor" href="#handling-missing-data"></a>
</h3>
<p>Missing data occur in nearly all empirical data, despite the vigorous efforts of researchers to prevent it. Missing data cause two general problems. First, if subjects with any missing data on the variables are removed from the dataset, the sample can be very small especially when the number of missing values is large. This can lead to a great loss of information and poor statistical power. Second, frequently the subjects who provide incomplete data are different from those who provide complete data. If adjustments are not made for these differences, results may be biased.</p>
<p>In the case of random missing-data mechanisms (i.e., ignorable missing data) such as missing completely at random (MCAR) and missing at random (MAR) (Little &amp; Rubin, 2019), two methods for dealing with missing data are typically available: full-information maximum likelihood (FIML) and multiple imputation (MI, Schafer, 1997). In MI plausible values are imputed multiple times in place of missing values to create multiple complete datasets. The use of MI for multiple-group LCA has an advantage in that missing data on covariates and group variable can be handled. However, the disadvantage is that LCA must be fitted separately for each imputed complete dataset, and the results must be combined to obtain the final estimates.
<!-- %An important advantage of using MI for multiple-group LCA is that missing data on covariates and grouping variable can be handled. However, the disadvantage is that LCA must be fitted within each imputed complete dataset, and the results must be combined to obtain final estimates.  -->
FIML is a model-based missing data procedure where model estimates are adjusted on the basis of all of the information provided by subjects with complete data and partially complete data. Most software packages for LCA employ a FIML approach because it requires no additional input from the user other than specifying that what code is used to denote missing data. However, this approach cannot handle missing data when missingness occurs in group variable or covariates in multiple-group LCA.</p>
<p>The package <code>glca</code> estimates model parameters using a FIML approach when some responses are found missing on manifest items: in E-step the missing responses are excluded from computing the posterior probability; and in M-step the indicator <span class="math inline">\(I(y_{igm} = k)\)</span> for the missing response is replaced with the updated <span class="math inline">\(\rho\)</span>-parameter from previous iteration. In short, the package <code>glca</code> can handle any ignorable missing data on manifest items, but individuals with missing data on group variable or any covariate are deleted from the analysis.
<!-- %\textcolor{orange}{The package `glca` handles the nonresponses on manifest items in a FIML approach; the missing responses are excluded from computing the posterior probability (in E-step), then the indicator $I(y_{igm} = k)$ for the missing response is replaced with the updated $\rho$-parameter from previous iteration(in M-step). In this way,} the package `glca` can handle any ignorable missing data on manifest items, but individuals with missing data on group variable or any covariate are deleted from the analysis.  -->
However, missing values on group variable and covariates can be treated using multiple multivariate imputation by chained equations (MICE, van Buuren et al., 2006), which is implemented in the R package <a href="https://CRAN.R-project.org/package=mice" class="external-link"><code>mice</code></a> (van Buuren &amp; Groothuis-Oudshoorn, 2011). MICE imputation could be used to create multiple sets of complete group variable and covariates for multiple-group LCA. Each complete dataset can be analyzed using the package <code>glca</code> and combining results across imputed datasets are easily obtained.</p>
</div>
<div class="section level3">
<h3 id="finding-global-maximum">Finding global maximum<a class="anchor" aria-label="anchor" href="#finding-global-maximum"></a>
</h3>
<p>Since the log-likelihood of LCA may have several local-maxima problem, the estimated parameters from EM algorithm can be deviated from the globally optimal solution. To cope with this problem, we recommend starting the algorithm using several different initial sets of random values and ascertaining whether they consistently converge to the same solution. If they converge, the solution can be considered as the ML estimates. If not, we recommend examining the distribution of the likelihood values and selecting the largest likelihood value, which usually corresponds to the ML solution.
The package <code>glca</code> allows investigators to try different starting values either by using random starting values or providing their own starting values. An investigator can select the number of initial sets of random values (default is 10) in the package <code>glca</code>, and then the package iterates EM algorithm a small number of times (default is 50) for each set of random values. Among the initial sets of model parameters, those producing the largest value of likelihood will be chosen for the main iteration.</p>
</div>
<div class="section level3">
<h3 id="standard-error-calculation">Standard error calculation<a class="anchor" aria-label="anchor" href="#standard-error-calculation"></a>
</h3>
<p>The standard error of the estimated parameters can be calculated using the observed empirical information matrix (Mclachlan &amp; Krishnan, 2007, p.Â 114),
<!--------------->
<span class="math display">\[
\boldsymbol{I}_{e}(\hat{\boldsymbol{\Psi}};\mathbf{Y}) =
\sum_{g = 1}^{G}\mathbf{s}(\mathbf{Y}_{g}; \hat{\boldsymbol{\Psi}})\mathbf{s}^\top(\mathbf{Y}_{g}; \hat{\boldsymbol{\Psi}}),
\]</span>
<!--------------->
where <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \hat{\boldsymbol{\Psi}})\)</span> is the score function of the parameter vector <span class="math inline">\(\boldsymbol{\Psi}\)</span> for the <span class="math inline">\(g\)</span>th group, evaluated at their MLE <span class="math inline">\(\hat{\boldsymbol{\Psi}}\)</span>. In the parameter vector <span class="math inline">\(\boldsymbol{\Psi}\)</span>, all probability parameters such as <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\rho\)</span>-parameters are transformed into free parameters using baseline logit function. The variance of <span class="math inline">\(\hat{\boldsymbol{\Psi}}\)</span> can be obtained by the inverse of <span class="math inline">\(\boldsymbol{I}_{e}(\hat{\boldsymbol{\Psi}};\mathbf{Y})\)</span>. However, as our target parameters are a re-parameterized version of <span class="math inline">\(\boldsymbol{\Psi}\)</span>, we should apply delta method to the variance of <span class="math inline">\(\hat{\boldsymbol{\Psi}}\)</span>. Let <span class="math inline">\(q(\boldsymbol{\Psi})\)</span> denote the original parameters of multiple-group latent class models. Then, the variance-covariance matrix of the estimates is
<!--------------->
<span class="math display">\[
\textsf{Var}\left(q(\hat{\boldsymbol{\Psi}})\right) =
J_q(\hat{\boldsymbol{\Psi}})\textsf{Var}(\hat{\boldsymbol{\Psi}})J_q(\hat{\boldsymbol{\Psi}})^\top,
\]</span>
<!--------------->
where <span class="math inline">\(J_q(\hat{\boldsymbol{\Psi}})\)</span> is the Jacobian matrix of the function <span class="math inline">\(q()\)</span> evaluated at the MLE of <span class="math inline">\(\boldsymbol{\Psi}\)</span>. Details of the score functions and the Jacobian matrices are provided in Appendix.</p>
</div>
</div>
<div class="section level2">
<h2 id="model-assessment-and-selection">Model assessment and selection<a class="anchor" aria-label="anchor" href="#model-assessment-and-selection"></a>
</h2>
<div class="section level3">
<h3 id="assessing-absolute-model-fit-for-measuring-goodness-of-fit">Assessing absolute model fit for measuring goodness-of-fit<a class="anchor" aria-label="anchor" href="#assessing-absolute-model-fit-for-measuring-goodness-of-fit"></a>
</h3>
<p>Absolute model fit refers to whether a specified multiple-group latent class model provides an adequate representation of the data. Typically, the analyst assesses absolute model fit by fitting a particular model to the observed data and testing the null hypothesis that the observed data has been produced by the fitted model. Thus, one usually hopes to find a model for which the null hypothesis is not rejected. This hypothesis test for LCA is based on a contingency table; the expected cell counts are estimated according to the specified model and its estimated parameters, then compared to the observed cell counts. The likelihood-ratio statistic, <span class="math inline">\(G^2\)</span> (Agresti, 2013) is used to assess absolute model fit in the package <code>glca.</code> The <span class="math inline">\(G^2\)</span> test statistic is derived from the difference in the log-likelihood values between the fitted model and the saturated model (i.e., expected and observed cell counts, respectively), where the residual degree of freedom is calculated by subtracting the number of parameters in the fitted model from those in the saturated model. The number of parameters for the saturated model is the lesser of number of possible combinations of categorical variables and number of cases in the model.</p>
<p>It should be noted that the contingency table for the LCA type of model is commonly sparse. When there are many cells containing very few observations in the cross-classification table, the large-sample approximation to the chi-square distribution for the <span class="math inline">\(G^2\)</span> statistic is not appropriate. In such case, the package <code>glca</code> allows us to conduct goodness-of-fit test using the bootstrap likelihood-ratio test (BLRT) statistic (Langeheine et al., 1996). This approach generates random datasets multiple times using the estimated parameters and calculates the <span class="math inline">\(G^2\)</span> statistic for each generated dataset. In BLRT the resulting distribution of the <span class="math inline">\(G^2\)</span> statistic across the random datasets is used as the reference distribution. The relative position of the <span class="math inline">\(G^2\)</span> statistic obtained from the original dataset within the reference distribution can be used as a measure of absolute model fit. In fact, the right tail probability of the observed <span class="math inline">\(G^2\)</span> value is regarded as a bootstrap <span class="math inline">\(p\)</span>-value. For example, if the observed <span class="math inline">\(G^2\)</span> value falls in the uppermost tail of the reference distribution, we may conclude that this test statistic is unlikely observed under the model corresponding to the null hypothesis. Such finding would provide evidence to reject the null hypothesis.</p>
</div>
<div class="section level3">
<h3 id="assessing-relative-model-fit-for-exploring-group-differences">Assessing relative model fit for exploring group differences<a class="anchor" aria-label="anchor" href="#assessing-relative-model-fit-for-exploring-group-differences"></a>
</h3>
<p>When comparing two or more groups in multiple-group latent class model, it should be checked if the latent features are identical or not across groups. The relative model fit refers to deciding which of two or more models represents a better fit to a particular dataset. The measurement invariance in multiple-group LCA can be tested by comparing the model fits of constrained versus unconstrained model; the unconstrained (full) model allows all parameters to vary across groups, while the constrained (reduced) model allows only the class prevalences to vary but item-response probabilities to be equal across groups. The package <code>glca</code> conducts the chi-square likelihood-ratio test (LRT) to assess relative model fit by comparing two competing models for testing measurement invariance.</p>
<p>Similar to the item-response probabilities, the coefficients for the level-1 covariates can also be tested for equality across groups using chi-square LRT in the package <code>glca.</code> By comparing the fit of reduced model with the coefficients held constant across groups (i.e., <span class="math inline">\(\boldsymbol{\beta}_{c} = \boldsymbol{\beta}_{c \mid 1}= \cdots = \boldsymbol{\beta}_{c \mid G}\)</span> in mgLCR and <span class="math inline">\(\boldsymbol{\beta}_{1c} = \boldsymbol{\beta}_{1c \mid 1} = \cdots = \boldsymbol{\beta}_{1c \mid W}\)</span> in npLCR for <span class="math inline">\(c=1,\ldots,C\)</span>) against full model with freely varying coefficients, we obtain evidence on whether the effects of level-1 covariates on latent class prevalences can be assumed to be identical across groups. To make the group comparison for coefficients valid, the assumption of measurement invariance must be met to ensure consistent meaning of latent classes across groups.</p>
<p>The deviance statistic, a test statistic of LRT for relative model fit, is obtained by twice the difference in the log-likelihood values of two competing models. The degree of freedom for deviance statistic is the difference in the number of free parameters of the two multiple-group latent class models. For example, the validity of the measurement invariance assumption can be tested by calculating the log-likelihood from the model where item-response probabilities are constrained to be equal across subgroups and comparing it with the log-likelihood from the freely estimated model.</p>
</div>
<div class="section level3">
<h3 id="choosing-the-numbers-of-latent-classes-and-latent-clusters">Choosing the numbers of latent classes and latent clusters<a class="anchor" aria-label="anchor" href="#choosing-the-numbers-of-latent-classes-and-latent-clusters"></a>
</h3>
<p>The chi-square LRT cannot be used to compare latent class models with a different number of latent classes or clusters because these two models are not nested. Thus, the package <code>glca</code> provides several information criteria commonly used in LCA such as Akaikeâ€™s information criterion (AIC), Bozdoganâ€™s criterion (CAIC), and Schwartzâ€™s Bayesian information criterion (BIC) (Akaike, 1974; Bozdogan, 1987; Schwarz, 1978) to compare the fit of non-nested competing models; the model with a smaller AIC (or BIC) value is preferred. Another model fit index provided by the package is entropy, which is widely used in research practices although it can be a poor measure for model selection as it often depends on the number of classes (Collins &amp; Lanza, 2009). The model with relatively higher entropy value is preferred.</p>
<p>The package <code>glca</code> also generates the empirical distribution of the deviance statistic to help select a better model between two non-nested competing models with a different number of latent classes or clusters using BLRT (Langeheine et al., 1996). The null hypothesis is the simpler model is adequate. Thus, the bootstrap sample will be drawn from the simpler model. Using a generated bootstrap sample, both competing models are estimated and the deviance between these two models is calculated. By repeating this procedure multiple times, we can construct the reference distribution of the deviance. Similar to the bootstrap <span class="math inline">\(p\)</span>-value for the <span class="math inline">\(G^2\)</span> statistic, the relative position of observed deviance within the reference distribution presents bootstrap <span class="math inline">\(p\)</span>-value; the null model with a bootstrap <span class="math inline">\(p\)</span>-value &gt; 0.05 is preferred with a significance level of <span class="math inline">\(\alpha = 0.05\)</span>. An important advantage of using BLRT is that this method can be applied to the test for comparing two nested latent class models even when the condition for large-sample approximation is not satisfied. It should be noted that the optimal model should be selected by comprehensively considering both conceptual and analytical implications, and the quantitative goodness-of-fit statistics.</p>
</div>
</div>
<div class="section level2">
<h2 id="appendix">Appendix<a class="anchor" aria-label="anchor" href="#appendix"></a>
</h2>
<div class="section level3">
<h3 id="score-functions-and-jacobian-matrices">Score functions and Jacobian matrices<a class="anchor" aria-label="anchor" href="#score-functions-and-jacobian-matrices"></a>
</h3>
<p><strong>Fixed-effect latent class analysis:</strong></p>
<p>Let <span class="math inline">\(\boldsymbol{\alpha}_g\)</span> and <span class="math inline">\(\boldsymbol{\beta}_g\)</span> be vectorized parameters containing all coefficients of <span class="math inline">\(\alpha_{c \mid g}\)</span> and <span class="math inline">\(\boldsymbol{\beta}_{c \mid g}\)</span> given in <a href="#eq:mgLCA-reg">(2)</a> for <span class="math inline">\(c=1,\ldots,C-1\)</span> and <span class="math inline">\(g=1,\ldots,G\)</span>, respectively. Further, let <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\alpha}_g)\)</span> and <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\beta}_g)\)</span> denote score functions of <span class="math inline">\(\boldsymbol{\alpha}_g\)</span> and <span class="math inline">\(\boldsymbol{\beta}_g\)</span>, respectively. Then, the element of <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\alpha}_g)\)</span> and the <span class="math inline">\(p\times 1\)</span> sub-vector of <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\beta}_g)\)</span> are obtained as
<!--------------->
<span class="math display" id="eq:score-reg-mglcr">\[\begin{eqnarray}
\tag{11}
\frac{\partial \log \mathcal{L}_{g}}{\partial \alpha_{c \mid g}} = \sum_{i=1}^{n_g} \left[ \theta_{ig(c)} - \gamma_{c \mid g}(\mathbf{x}_{ig}) \right]
\;\; \mbox{and} \;\;
\frac{\partial \log \mathcal{L}_{g}}{\partial \boldsymbol{\beta}_{c \mid g}} = \sum_{i=1}^{n_g} \left[ \mathbf{x}_{ig} \left(\theta_{ig(c)} - \gamma_{c \mid g}(\mathbf{x}_{ig})\right) \right]
\end{eqnarray}\]</span>
<!--------------->
for <span class="math inline">\(c=1, \ldots, C-1\)</span> and <span class="math inline">\(g=1,\ldots,G\)</span>, respectively. Note that <span class="math inline">\(\mathcal{L}_{g}\)</span> in <a href="#eq:score-reg-mglcr">(11)</a> is the product of observed-data likelihoods given in <a href="#eq:likeli">(1)</a> for all observations in the <span class="math inline">\(g\)</span>th group (i.e., <span class="math inline">\(\mathcal{L}_{g}=\prod_{i=1}^{n_g}\mathcal{L}_{ig}\)</span>).</p>
<p>Let <span class="math inline">\(\boldsymbol{\rho}_g\)</span> denote vectorized item-response probabilities containing all <span class="math inline">\(\boldsymbol{\rho}_{m \mid cg}=(\rho_{m1 \mid cg}, \ldots, \rho_{mr_m \mid cg})^\top\)</span> for <span class="math inline">\(m=1,\ldots,M\)</span>, <span class="math inline">\(c=1,\ldots,C\)</span> and <span class="math inline">\(g=1,\ldots,G\)</span>. Each of <span class="math inline">\(\rho\)</span>-parameters in <span class="math inline">\(\boldsymbol{\rho}_{m \mid cg}\)</span> is re-parameterized by the baseline logit function <span class="math inline">\(\pi_{mk \mid cg} = \ln(\rho_{mk \mid cg} / \rho_{mr_m \mid cg})\)</span> for <span class="math inline">\(k=1,\ldots, r_m-1\)</span>, and let <span class="math inline">\(\boldsymbol{\pi}_g\)</span> denote vectorized free parameters containing all <span class="math inline">\(\boldsymbol{\pi}_{m \mid cg}=(\pi_{m1 \mid cg}, \ldots, \pi_{mr_m-1 \mid cg})^\top\)</span> for <span class="math inline">\(m=1,\ldots,M\)</span>, <span class="math inline">\(c=1,\ldots,C\)</span> and <span class="math inline">\(g=1,\ldots,G\)</span>. Further, let <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\pi}_g)\)</span> denote score function of all free parameters <span class="math inline">\(\boldsymbol{\pi}_g\)</span>. Then, the element of score function <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\pi}_g)\)</span> is obtained as
<!--------------->
<span class="math display" id="eq:score-pi">\[\begin{eqnarray}
\tag{12}
\frac{\partial \log \mathcal{L}_{g}}{\partial \pi_{mk \mid cg}} = \sum_{i=1}^{n_g} \left[\theta_{ig(c)} \left(I(y_{igm} = k) - \rho_{mk\mid cg}\right) \right]
\end{eqnarray}\]</span>
<!--------------->
for <span class="math inline">\(k=1,\ldots, r_m-1\)</span>, <span class="math inline">\(m=1, \ldots, M\)</span>, <span class="math inline">\(c=1, \ldots, C\)</span>, and <span class="math inline">\(g=1,\ldots,G\)</span>.</p>
<p>Let <span class="math inline">\(\boldsymbol{\Psi}\)</span> denote vector for all free parameters <span class="math inline">\(\boldsymbol{\alpha}=(\boldsymbol{\alpha}_1^\top, \ldots, \boldsymbol{\alpha}_G^\top)^\top\)</span>, <span class="math inline">\(\boldsymbol{\beta}=(\boldsymbol{\beta}_1^\top, \ldots, \boldsymbol{\beta}_G^\top)^\top\)</span>, and <span class="math inline">\(\boldsymbol{\pi}=(\boldsymbol{\pi}_1^\top, \ldots, \boldsymbol{\pi}_G^\top)^\top\)</span>, and let <span class="math inline">\(q(\boldsymbol{\Psi})\)</span> be the function to transform back to the original parameters of mgLCR. Then, the Jacobian matrix for the function <span class="math inline">\(q(\boldsymbol{\Psi})\)</span> is
<!--------------->
<span class="math display" id="eq:Jacobian-mglcr">\[\begin{eqnarray}
\tag{13}
J_q(\boldsymbol{\Psi}) = \begin{bmatrix}
  J_q(\boldsymbol{\alpha}, \boldsymbol{\beta}) &amp; \mathbf{0} \\
  \mathbf{0} &amp; J_q(\boldsymbol{\pi})
\end{bmatrix},
\end{eqnarray}\]</span>
<!--------------->
where <span class="math inline">\(J_q(\boldsymbol{\alpha}, \boldsymbol{\beta})\)</span> is an identity matrix of size equal to the number of regression coefficients <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span>. The sub-matrix of <span class="math inline">\(J_q(\boldsymbol{\pi})\)</span> given in <a href="#eq:Jacobian-mglcr">(13)</a> can be specified by <span class="math inline">\({\partial \boldsymbol{\rho}_{m\mid cg}}/{\partial \boldsymbol{\pi}_{m'\mid c'g'}}\)</span>, which is the matrix of size <span class="math inline">\(r_m \times (r_{m'}-1)\)</span> for <span class="math inline">\(m\)</span>, <span class="math inline">\(m'=1, \ldots, M\)</span>; <span class="math inline">\(c\)</span>, <span class="math inline">\(c'=1, \ldots, C\)</span>; and <span class="math inline">\(g\)</span>, <span class="math inline">\(g'=1,\ldots,G\)</span>. The element of this sub-matrix in the <span class="math inline">\(k\)</span>th row and the <span class="math inline">\(k'\)</span>th column can be obtained as
<!--------------->
<span class="math display" id="eq:Jacobian-rho">\[\begin{eqnarray}
\tag{14}
\frac{\partial {\rho}_{mk\mid cg}}{\partial {\pi}_{m'k'\mid c'g'}} =
I(g = g') I(c = c') I(m = m') \rho_{mk\mid cg} \left(I(k = k') - \rho_{m'k'\mid c'g'}\right)
\end{eqnarray}\]</span>
<!--------------->
for <span class="math inline">\(k = 1,\ldots, r_m\)</span> and <span class="math inline">\(k' = 1,\ldots, r_m-1\)</span>.</p>
<p><strong>Random-effect latent class analysis:</strong></p>
<p>The prevalences for latent clusters, <span class="math inline">\(\boldsymbol{\delta} = (\delta_1, \ldots, \delta_W)^\top\)</span> are re-parametrized by the baseline logit function <span class="math inline">\(\zeta_w = \ln\left(\delta_w / \delta_W\right)\)</span> for <span class="math inline">\(w=1,\ldots,W-1\)</span>. Let <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\zeta})\)</span> denote score function of <span class="math inline">\(\boldsymbol{\zeta}=(\zeta_1, \ldots, \zeta_{W-1})^\top\)</span> for the <span class="math inline">\(g\)</span>th group. Then, the <span class="math inline">\(w\)</span>th element of <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\zeta})\)</span> is obtained as
<!--------------->
<span class="math display" id="eq:score-delta">\[\begin{eqnarray*}
\tag{15}
\frac{\partial \log \mathcal{L}_{g}}{\partial \delta_w} = \theta_{g(w)} - \delta_{w}
\end{eqnarray*}\]</span>
<!--------------->
for <span class="math inline">\(w=1,\ldots,W-1\)</span>, where <span class="math inline">\(\mathcal{L}_{g}\)</span> is the observed-data likelihood of npLCR for the <span class="math inline">\(g\)</span>th group given in <a href="#eq:grouploglik-mLCA">(4)</a>.</p>
<p>Let <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{\beta}_1\)</span> be vectorized parameters containing all coefficients of level-1 covariates, <span class="math inline">\(\alpha_{c \mid w}\)</span> and <span class="math inline">\(\boldsymbol{\beta}_{1c \mid w}\)</span> given in <a href="#eq:MLCA-reg">(5)</a> for <span class="math inline">\(c=1,\ldots,C-1\)</span> and <span class="math inline">\(w=1,\ldots,W\)</span>, respectively.
Further, let <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\alpha})\)</span> and <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\beta}_1)\)</span> denote score functions of <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{\beta}_1\)</span> for the <span class="math inline">\(g\)</span>th group, respectively. Then, the element of <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\alpha})\)</span> and the <span class="math inline">\(p\times 1\)</span> sub-vector of <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\beta}_1)\)</span> are obtained as
<!--------------->
<span class="math display">\[\begin{eqnarray*}
\frac{\partial \log \mathcal{L}_{g}}{\partial \alpha_{c \mid w}} &amp;=&amp; \sum_{i=1}^{n_g} \left[ \theta_{ig(c)} - \theta_{g(w)}\gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_{g}) \right]
\;\; \mbox {and} \\
\frac{\partial \log \mathcal{L}_{g}}{\partial \boldsymbol{\beta}_{1c \mid w}} &amp;=&amp; \sum_{i=1}^{n_g} \left[ \mathbf{x}_{ig}\left(\theta_{ig(c)} - \theta_{g(w)}\gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_{g})\right) \right]
\end{eqnarray*}\]</span>
<!--------------->
for <span class="math inline">\(c=1, \ldots, C-1\)</span>, <span class="math inline">\(w=1,\ldots,W\)</span>, and <span class="math inline">\(g=1, \ldots, G\)</span>, respectively. Let <span class="math inline">\(\boldsymbol{\beta}_2\)</span> denote vectorized parameters containing all coefficients of level-2 covariates, <span class="math inline">\(\boldsymbol{\beta}_{2c}\)</span> given in <a href="#eq:MLCA-reg">(5)</a> for <span class="math inline">\(c=1,\ldots,C-1\)</span>. The <span class="math inline">\(q \times 1\)</span> sub-vector of
score function for <span class="math inline">\(\boldsymbol{\beta}_{2}\)</span>, <span class="math inline">\(\mathbf{s}(\mathbf{Y}_{g}; \boldsymbol{\beta}_{2})\)</span> can be obtained as
<!--------------->
<span class="math display">\[\begin{eqnarray*}
\frac{\partial \log \mathcal{L}_{g}}{\partial \boldsymbol{\beta}_{2c}} = \mathbf{z}_{g} \sum_{i=1}^{n_g} \left[\theta_{ig(c)} - \sum_{w = 1}^{W} \theta_{g(w)} \gamma_{c \mid w}(\mathbf{x}_{ig}, \mathbf{z}_{g})\right]
\end{eqnarray*}\]</span>
<!--------------->
for <span class="math inline">\(c=1, \ldots, C-1\)</span> and <span class="math inline">\(g=1, \ldots, G\)</span>. The score functions for the free parameters of item-response probabilities are identical to mgLCR given in <a href="#eq:score-pi">(12)</a>.</p>
<p>Let <span class="math inline">\(\boldsymbol{\Psi}\)</span> denote vector for all free parameters <span class="math inline">\(\boldsymbol{\zeta}\)</span>, <span class="math inline">\(\boldsymbol{\alpha}\)</span>, <span class="math inline">\(\boldsymbol{\beta}_1\)</span>, <span class="math inline">\(\boldsymbol{\beta}_2\)</span>, and <span class="math inline">\(\boldsymbol{\pi}\)</span>, and let <span class="math inline">\(q(\boldsymbol{\Psi})\)</span> be the function to transform back to the original parameters of npLCR. Then, the Jacobian matrix for the function <span class="math inline">\(q(\boldsymbol{\Psi})\)</span> is
<!--------------->
<span class="math display">\[
J_q(\boldsymbol{\Psi}) = \begin{bmatrix}
  J_g(\boldsymbol{\zeta}) &amp; \mathbf{0} &amp; \mathbf{0} \\
  \mathbf{0} &amp; J_q(\boldsymbol{\alpha}, \boldsymbol{\beta}_1, \boldsymbol{\beta}_2) &amp; \mathbf{0} \\
  \mathbf{0} &amp; \mathbf{0} &amp; J_q(\boldsymbol{\pi})
\end{bmatrix},
\]</span>
<!--------------->
where <span class="math inline">\(J(\boldsymbol{\alpha}, \boldsymbol{\beta}_1, \boldsymbol{\beta}_2)\)</span> is an identity matrix of size equal to the number of regression coefficients given in <a href="#eq:MLCA-reg">(5)</a>. The sub-matrix of <span class="math inline">\(J_q(\boldsymbol{\pi})\)</span> are identical to those given in <a href="#eq:Jacobian-rho">(14)</a>, an the elements of <span class="math inline">\(J_q(\boldsymbol{\zeta})\)</span> can be obtained by
<!--------------->
<span class="math display">\[
\begin{aligned}
\frac{\partial \delta_{w}}{\partial \zeta_{w'}} = \delta_{w} \left(I(w = w') - \delta_{w'}\right)
\end{aligned}
\]</span>
<!--------------->
for <span class="math inline">\(w=1,\ldots,W\)</span> and <span class="math inline">\(w'=1,\ldots,W-1\)</span>.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Youngsun Kim, Hwan Chung.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
